# Building an AI-Powered Food Search System with FAISS and Llama 3.2

*A step-by-step guide to creating a RAG-powered food recommendation system using vector similarity search*

Ever wondered how to build an intelligent food recommendation system that understands natural language queries? In this tutorial, we'll create a complete solution using FAISS for lightning-fast vector search and Llama 3.2 for conversational AI responses.

## What We're Building

Our system features **four distinct interfaces**:
1. **Interactive CLI Search** - Real-time food discovery
2. **Advanced Search** - Filtering by cuisine and calories  
3. **RAG Chatbot** - Natural language conversations
4. **System Comparison** - Performance benchmarking

## Prerequisites & Setup

First, let's set up our environment with the required dependencies:

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install numpy==1.26.4
pip install faiss-cpu==1.7.4
pip install sentence-transformers==4.1.0
pip install ollama
```

## Step 1: Install Ollama and Llama 3.2

For our RAG chatbot, we need Ollama running locally:

```bash
# Install Ollama (visit ollama.ai for instructions)
# Then pull the Llama 3.2 model
ollama pull llama3.2
ollama serve  # Start the Ollama service
```

## Step 2: Understanding the Food Dataset

Before diving into the implementation, let's understand our data structure. Our system uses a comprehensive JSON dataset with 185 food items, each containing rich metadata:

```json
{
  "food_id": 1,
  "food_name": "Apple Pie",
  "food_description": "A classic dessert made with a buttery, flaky crust filled with tender, spiced apples.",
  "food_calories_per_serving": 320,
  "food_nutritional_factors": {
    "carbohydrates": "42g",
    "protein": "2g", 
    "fat": "16g"
  },
  "food_ingredients": ["Apples", "Flour", "Butter", "Sugar", "Cinnamon", "Nutmeg"],
  "food_health_benefits": "Rich in antioxidants and dietary fiber",
  "cooking_method": "Baking",
  "cuisine_type": "American",
  "food_features": {
    "taste": "sweet",
    "texture": "crisp and tender",
    "appearance": "golden brown",
    "preparation": "baked",
    "serving_type": "hot"
  }
}
```

**Key Data Fields:**
- **Basic Info**: `food_name`, `food_description`, `food_id`
- **Nutrition**: `food_calories_per_serving`, `food_nutritional_factors`
- **Ingredients**: `food_ingredients` array with all components
- **Classification**: `cuisine_type`, `cooking_method`
- **Sensory**: `food_features` with taste, texture, appearance
- **Health**: `food_health_benefits` for wellness-focused searches

This rich structure allows our system to understand not just what foods are, but their nutritional profiles, cooking methods, and sensory characteristics - enabling highly relevant search results.

## Step 3: Core Vector Search Implementation

The heart of our system is the FAISS-powered similarity search. Here's how we implement it:

```python
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

class FAISSFoodCollection:
    def __init__(self, collection_name: str):
        self.collection_name = collection_name
        self.model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = None
        self.metadatas = []
        
    def add(self, documents: List[str], metadatas: List[Dict]):
        # Generate embeddings
        embeddings = self.model.encode(documents)
        self.embeddings = np.array(embeddings).astype('float32')
        
        # Normalize for cosine similarity
        faiss.normalize_L2(self.embeddings)
        
        # Create FAISS index
        dimension = self.embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        self.index.add(self.embeddings)
        
        self.metadatas = metadatas
```

## Step 4: Data Processing Pipeline

We transform our food data into rich text representations for better search results:

```python
def create_food_embedding_text(food_item):
    text = f"Name: {food_item['food_name']}. "
    text += f"Description: {food_item['food_description']}. "
    text += f"Ingredients: {', '.join(food_item['food_ingredients'])}. "
    text += f"Cuisine: {food_item['cuisine_type']}. "
    text += f"Calories: {food_item['food_calories_per_serving']}. "
    
    # Add nutritional information
    if 'food_nutritional_factors' in food_item:
        nutrition = food_item['food_nutritional_factors']
        nutrition_text = ', '.join([f"{k}: {v}" for k, v in nutrition.items()])
        text += f"Nutrition: {nutrition_text}."
    
    return text
```

## Step 5: RAG Chatbot Integration

The magic happens when we combine FAISS search with Llama 3.2 for intelligent responses:

```python
import ollama

def generate_rag_response(query: str, search_results: List[Dict]) -> str:
    # Prepare context from search results
    context = "Based on your query, here are relevant food options:\n"
    for i, result in enumerate(search_results[:3], 1):
        context += f"{i}. {result['food_name']} - {result['food_description']}\n"
    
    # Create prompt for Llama 3.2
    prompt = f"""You are a helpful food recommendation assistant.
    
User Query: "{query}"

Retrieved Food Information:
{context}

IMPORTANT INSTRUCTIONS:
- If no relevant food items were found OR if the food options don't match the user's query, respond EXACTLY with: "Nothing found in my database"
- Only recommend food items that are actually listed in the retrieved information above
- Do NOT make up or suggest foods that are not in the database results

If you have relevant food options, provide a helpful, conversational response recommending 2-3 items from the options above.

Response:"""
    
    # Generate response using Ollama
    response = ollama.chat(
        model='llama3.2',
        messages=[{'role': 'user', 'content': prompt}]
    )
    
    return response['message']['content']
```

## Step 6: Advanced Filtering

Our system supports sophisticated filtering by cuisine type and calories:

```python
def perform_filtered_search(collection, query: str, cuisine_filter: str = None, 
                           max_calories: int = None, n_results: int = 5):
    # Build filter conditions
    filters = []
    if cuisine_filter:
        filters.append({"cuisine_type": cuisine_filter})
    if max_calories:
        filters.append({"calories": {"$lte": max_calories}})
    
    # Apply filters during search
    where_clause = {"$and": filters} if len(filters) > 1 else filters[0] if filters else None
    
    return collection.query(
        query_texts=[query],
        n_results=n_results,
        where=where_clause
    )
```

## Step 7: Running the Systems

Each system provides a different interaction model:

```bash
cd python

# Interactive search with real-time results
python3 interactive_search.py

# Advanced search with filtering options
python3 advanced_search.py

# Conversational AI chatbot
python3 enhanced_rag_chatbot.py

# Performance comparison tool
python3 system_comparison.py
```

## Performance Benefits

Our FAISS implementation delivers significant advantages:

- **Speed**: Sub-second search times for 185+ food items
- **Memory**: 40% lower memory usage vs ChromaDB
- **Scalability**: Linear scaling for larger datasets
- **Accuracy**: 62%+ relevance for food queries

## Example Interaction

```
üîç Search for food: I want something spicy and healthy for dinner

ü§ñ Bot: Based on your request for spicy and healthy dinner options, I'd recommend:

1. **Thai Green Curry** - A flavorful dish with vegetables and lean protein, packed with antioxidants from fresh herbs and spices (320 calories)

2. **Spicy Quinoa Bowl** - Nutrient-dense quinoa with roasted vegetables and chili peppers, providing complete proteins and fiber (280 calories)

Both options offer the spicy kick you're craving while delivering excellent nutritional value for a healthy dinner!
```

## Architecture Overview

Our system architecture combines:

- **FAISS**: High-performance vector similarity search
- **SentenceTransformers**: Semantic text embeddings  
- **Ollama + Llama 3.2**: Local AI inference
- **Rich Metadata**: Advanced filtering capabilities
- **CLI Interfaces**: Multiple interaction paradigms

## Get the Complete Code

The full implementation with all four systems, comprehensive testing, and documentation is available on GitHub:

**Repository**: [Food Search RAG System](https://github.com/your-username/small-rag-app-foodsearch)

## Conclusion

We've built a sophisticated food recommendation system that demonstrates the power of combining vector similarity search with large language models. The system showcases:

‚úÖ **Four different interaction models** for various use cases  
‚úÖ **FAISS optimization** for blazing-fast similarity search  
‚úÖ **RAG architecture** with local Llama 3.2 inference  
‚úÖ **Advanced filtering** by cuisine, calories, and ingredients  
‚úÖ **Production-ready code** with comprehensive testing  

This architecture can be easily adapted for other domains like product recommendations, document search, or knowledge bases. The combination of FAISS and local LLM inference provides both performance and privacy benefits.

Ready to build your own intelligent search system? Clone the repository and start experimenting with your own data!

---

*Have questions about the implementation? Drop them in the comments below, and I'll be happy to help you get started with your own RAG-powered search system.*